Id,ForumTopicId,AuthorUserId,PostDate,Message,ReplyToForumMessageId,TopicMessagePosition,RawMarkdown,Score,FlaggedCount
1,1,478,"2010-04-28 23:13:08","<div>In response to a comment on the No Free Hunch Blog <a href=""http://kaggle.com/blog/2010/04/27/beating-up-on-hiv/"" mce_href=""http://kaggle.com/blog/2010/04/27/beating-up-on-hiv/"">here </a>and on my website <a mce_href=""http://www.willdampier.info/2010/a-few-ideas/"" href=""http://www.willdampier.info/2010/a-few-ideas/"">here</a> ... I'm listing a few ideas that may give you a head-start on how to get started:</div><div><br></div><div>There are a whole lotta ways to do this sort of prediction, since I come from a machine-learning background I'll describe it from that perspective: &nbsp;So in my mind I need to extract a set of features (observations) from each sequences, then train a SVM, logistic-regression, decision-forest, or ensemble classifier to learn which features are important. &nbsp;Each of those classifiers have advantages and disadvantages that are very well documented ... a safari through Wikipedia should give you a pretty good idea.</div><div><br></div><div>The hardest part is deciding which features are worth (or even possible) to put into your model:</div><div><br></div><div>I know people who use ""k-mers"" as their features ... this involves finding and counting all of the 5 letter instances in the sequence. &nbsp;Then you can use these as features in a prediction model. &nbsp;K-mers are nice because they are easy to pull out with any programming language you can think of. There is also a list of regular-expressions which have some biological meaning here: http://elm.eu.org/browse.html</div><div><br></div><div>Other people prefer to use the raw sequence. &nbsp;If you can align the sequences (since they don't all start at the same part of the gene) using a program like ClustalW then you can think of each column as a categorical feature. &nbsp;The problem here is that HIV-1 is highly variable and alignments are difficult ... although not impossible.</div><div><br></div><div>If you wander around the Los Alamos HIV-1 database you can find a list of known resistance mutations: http://www.hiv.lanl.gov/content/sequence/RESDB/. &nbsp;These have been verified to be important in the viral resistance to certain drugs. &nbsp;You can use the presence or absence of these mutations as features to train a model.</div><div><br></div><div>I'm sure there are dozens of ways to extract features that I've never even heard of so don't think that these are your only choices.</div>",,1.0,,0,0
76,1,808,"2010-04-28 23:13:08","Use a mixture of different models (linear regression, neural networks). Choosing the best model by using the wave criterion. The theoretical grounding of the criterion is based on Bayes Theorem, the methods of cybernetics and synergy. See, article ""Performance criterion of neural networks learning"" published in the ""Optical Memory &amp; Neural Networks"" Vol. 17, number&nbsp; 3. pp. 208-219.&nbsp; DOI:10.3103/S1060992X08030041<br>http://www.springerlink.com/content/t231300275038307/?p=0c94471924774e8894973ad3c0d391a7&amp;pi=0<br mce_bogus=""1"">",,2.0,,0,0
116,1,1486,"2010-04-28 23:13:08","My first thoughts on this problem are that it would be ideal for Geof Hinton's deep belief networks.<br><br>http://www.scholarpedia.org/article/Deep_belief_networks<br><br>They are generally competetive at tasks such as high level feature detection in images and audio. Quite possibly though there isn't enough data available to use this approach - not sure.<br><br>Is anyone using this approach?<br mce_bogus=""1"">",,3.0,,0,0
119,1,703,"2010-04-28 23:13:08","The theory of deep belief networks is probably a correct description of information encoded in the human brain, but it's not a complete theory.<br><br>As such, you will have difficulty using it to solve <b>any</b> problem, much less this one.<br><br>Deep belief network theory fails because it does not imply how many layers or variables there should be.<br><br>Certainly, someone could implement a network with a chosen number of layers and variables that ""seem right"". Depending on how well your model represents the correct encoding the results will show anywhere from random to perfect correlation.<br><br>...and there are many, many more configurations with random correlation than perfect correlation.<br><br>(That a wrong feature in the model will push the output towards randomness is the big number one reason why machine algorithms require a ton of data and then only show weak results. For comparison, the human brain will correlate from three samples and is usually correct.)<br><br>As for this particular problem, it's not clear that deeply hidden features with complex dependencies are even needed. Chances are basic observations about the data will be sufficient.<br><br>Additionally, from an information theoretic point of view, well over half of the information needed to find a correlation was moved into the test set where we can't see it. This makes finding correlations extremely difficult, and any which are found will probably be weak at best.<br><br>(Future events may prove me wrong, but I have good evidence for this last statement.)<br><br mce_bogus=""1"">",,4.0,,0,0
130,1,1486,"2010-04-28 23:13:08","Going by Hinton's last two google techtalks, recent results have been very strong. Yes the topology is manually selected, but that's not a show stopper, quite clearly less structure forces generalisation and one can plot the curve of how performance changes with # of nodes and use that to make estimated guesses about the approximate location of a sweet spot.

I take the point that deep features may not be useful here, but that's not a certainty. However, I do very much doubt there is enough data to find anything other than 'surface'  features.

On the last point - It was an interesting aspect of the netflix prize that folks were able to use the available test data in the training algorithm despite not having the hidden part of the data. I believe that all of the top teams integrated this data to improve their models of the data overall. I wonder if a similar approach could be used here. E.g. deep belief nets model the data initially without any need (in this HIV task) for the responder flag. For netflix this was perhaps an unexpected loophole which remained open (probably because the workaround is to submit an algorithm rather than predictions, which would have been unworkable).",,5.0,,0,0
131,1,703,"2010-04-28 23:13:08","I believe I understand deep belief nets well enough to implement one to look for correlations in the response data.<br><br>My first real submission was analogous to a belief net with two layers. It got very good results on the training data - around 68% on the first attempt - but failed miserably on the testing data.<br><br>After spinning my wheels for several weeks trying to tweak the model to better fit the data, I finally wrote some code to detect selection bias in the test set.<br><br>It turns out that the test set suffers from enormous selection bias, and this is why I believe that any model based on the training set will only loosely predict the test data.<br><br>Here's an example. Plotting RTrans length (actually, the ""tail"" length after the main body) shows a section which is 100% Non-Responded. There are 74 patients in this section, and the chances of this happening due to chance is less than 1 in 25 million.<br><br>Ordinarily this would be a strong indicator of patient response which should be incorporated into the model. There are 24 such patients in the test set, and if the correlation holds all should be set to non-respond.<br><br><br mce_bogus=""1"">

<img alt=""RTrans Length vs Responded"" src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/RTransLength.png"" mce_src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/RTransLength.png""><br><br><br>I don't have access to the test data, but I *believe* that selection bias has put all of the ""responded"" patients from this sub-population into the test set. Meaning, that there is no way to measure the relevance of this feature from the training data alone.<br><br>I have identified a couple of these anomalies.<br><br>My current system works within the selection bias and is having much 
better results.<br><br><br>",,6.0,,0,0
135,1,1486,"2010-04-28 23:13:08","Thanks Rajstennaj. That's a very odd bias, and given your experience with poor initial test results I don't doubt it. 
It would be interesting to get some feedback on this from the organisers, perhaps after the competition has finished.

I guess it's slightly unfortunate that the novel techniques required to compete on a task like this are overshadowed by the need to do some simpler statistical analyses before you get started proper, but that's real world data for you :)",,7.0,,0,0
148,1,1486,"2010-04-28 23:13:08","Rajstennaj, congrats on your progress so far. 76+ is a very promising and interesting score. Looking forward to learning more about your approach(es) after the end of the competition.",,8.0,,0,0
151,1,703,"2010-04-28 23:13:08","Heh.<br><br>I've got almost 10,000 lines of code so far in perl (a highly expressive language) and 6 separate models which don't work on the test data.<br><br>Don't get discouraged by my apparent MCE. It is entirely likely my approach will be declared invalid by the contest organizer. My system identifies weaknesses of the contest data rather than doing real prediction.<br><br>A more interesting result from the leaderboard is ""team sayani"", who got an extremely good score <b>in three attempts</b>. (!)<br><br>Alex (of ""team sayani"") sent me his paper outlining his method. I haven't read it yet - I'm waiting for the contest to end - but I bet he's the one to beat. When he comes back from vacation, that is.<br><br>Grzegorz Swirszcz sent me two of his papers - they were an interesting read. He's won two other data prediction competitions. His papers introduced me to ""ensemble selection"" - a concept of which I was unaware.<br><br>There's been very little discussion about the contest in general, but I know that the people here are accessible and easy to talk to. And there's lots of opportunity to learn new stuff by talking to people.<br><br>It's a pity people don't talk more. I wonder what techniques John Chouchoumis (team hcj) is using?<br><br mce_bogus=""1"">",,9.0,,0,0
152,1,673,"2010-04-28 23:13:08","Rajstennaj, 
I can't say I've spent as much time on this contest as you but it has become clear to me there is serious selction bias in the choice of training and test sets. Another obvious example: 80 of the traing set are missing PR data entirely - none of the test set are missing ALL PR data. I have also seen clusters of PR sequences in the test set that do not occur in the training set. 

This lack of random assignment to training and tests sets is disappointing. We are not looking just for features associated with reponse but also patterns of bias.",,10.0,,0,0
154,1,1486,"2010-04-28 23:13:08","Dividing the data into equally representative sets is always going to be problemtaic for such small numbers of records (in comparison to say the Netflix prize which had half a million user accounts and 17000 movies). 

I suppose my main concern surrounds how much overfitting can occur to the public portion of the test set (quite a lot it would seem) and whether this goes towards the final score. Strictly speaking we should only be scored on the hidden portion (which may be the case) since we have no idea what the biases are in that set and therefore cannot make assumptions. Of course, some may get lucky or may make educated guesses about the biases in that set",,11.0,,0,0
156,1,673,"2010-04-28 23:13:08","Hi Colin,
""Dividing the data into equally representative sets is always going to be problemtaic ...""
It just has to be randomised, or randomised subject to constraints on the numbers of cases/controls assigned to the test and training groups. Machin learning algorithms and statistical learning techniques rely on the training set being REPRESENTATIVE of the test set. Otherwise there's no point in learning.",,12.0,,0,0
157,1,673,"2010-04-28 23:13:08","<p>I'll briefly post a summary of my 20 hours (or so) of work on this. 
I'm a statistician so I prefer to stick to proven methods. <br></p><p><br></p><p>I've used logistic regression with stepwise variable selection and Baysian Information Criterion (BIC) to stop overfitting. I've used simple receiver operating characteristic (ROC) concepts to choose cut points. I've also used cross-validated my entire model building process using n-fold or leave-one-out cross validation. These are robust reliable techniques that: 1) almost always work, 2) prevent overfitting (statistics has a concept called ""inference"" that is enormously valuable here), 3) minimise effort, 4) provide a good guide to model performance on a new test dataset representative of the training dataset. I used the statistical package R for those who are interested.</p><p><br></p><p>

My conclusion is that the test dataset is NOT representative of the training dataset. I absolutely agree with Rajstennaj that the dataset partitioning was not random and is highly biased.

My first fitting attempts used only viral load and CD4 count (both modelled as logs). After using viral load, CD4 count is not a useful predictor. I chose cut-points on the ROC to maximise accuracy on the test dataset, where cases and controls are equally numerous (roughly where sensitivity + specificity is a maximum). I got an MCE (actually accuracy) of 62.5%.</p><p><br></p><p>

My next attempt was what took all the time. I'm not very used to translating sequences (or rather alighing them), and I discovered a few errors in my work through Cory's contributions. Thanks Cory! Then I simply looked for amino acids associated with disease outcome, created indicator variables for the various ""alleles"" at these sites and then ran the variable selection (keeping viral load and CD4 count as possible variables to choose from). My final models had only 8 variables, including viral load. At the appropriate cut-point they predicted an accuracy of 79% - of course this won't happen as the model is built on that dataset. But the cross-validated accuracy was 76% (projected to a dataset balanced in cases and controls). This is what statistical theory would predict I would get if the test dataset was representative of the training dataset. I again got an MCE of 62.5%. <br></p><p><br></p><p>Noting that the 80 patients with missing PR sequence, and ALL in the training dataset, could not have been randomly assigned, I deleted them and tried buiding the model again. I still got an MCE (accuracy) of 62.5%.</p><p><br></p><p> 

I am forced to the same conclusion as Rajstennaj. There is serious selection bias which makes the problem unsuitable for statistical or machine learning approaches. One is forced to ""go fishing"" for bias, and that means submitting solutions based on hypothesised bias and observing the outcome. Great as a game, but of little use in problems that concern me. <br></p><p><br></p><p>So in my view Rajstennaj deserves to win. It is just a shame that the kind of research paper that will come out of this is not the kind Will intended - nor can it be - as the dataset has not been randomly partitioned. <br></p><p><br></p><p>I suspect those who have done better with train-test approach (eg. sayani) have been in some senses lucky in that that chose feature-outcome sets more randomly distributed between the two datsets - viral load is one such predictor. It got me an accuracy (MCE) of 62.5% by itself.<br></p>",,13.0,,0,0
159,1,1486,"2010-04-28 23:13:08","Hi Bruce, I appreciate you describing your approaches and discoveries. It will be intersting to see what the outcome of the competition will be if there really are no other features (or significant features in comparison to the biases) in the data. Are there features that are consistent across test & training data that the leaders could be basing their predictions on? Probably not based on what I've read.

I think it's early days for kaggle and my hope would be that folks don't get disheartened - that the organisers can learn from this and improve the design of future competions. It's an iterative process. 

Also I'd be interested in learning how your (and others) progress on modelling the training data compares with the current best techniques described in the literature.

Colin.",,14.0,,0,0
198,1,1902,"2010-04-28 23:13:08","Training and test data are clearly hand-picked and/or biased somehow. It's like a ""trick question"" in a quiz. I can see how that makes a contest more challenging, but if the purpose is to produce useful results, this just hinders the task at hand.",,15.0,,0,0
205,1,728,"2010-04-28 23:13:08","Well, it's unfortunate that the contest turned out this way, but I *really* hope that when the contest is over, the top people on the leaderboard (Rajstennaj, Flying Pig, sayani) will find some way to test their algorithms in a way that's more comparable with the other literature on the subject.  For example, cross-validation on the whole set.&nbsp;<div><br></div><div>Hopefully those results could be posted or at least linked to on Kaggle -- I'm very curious how much of the increase in MCE here is merely an artifact and how much is a genuine advance in the field.  Rajstennaj's MCE in particular would be astonishing if anywhere close to accurate.</div>",,16.0,,0,0
206,1,368,"2010-04-28 23:13:08","The public leaderboard is only indicative because competitors can use information on their score to get information on a portion of the test dataset. The final results are a) quite different and b) better reflect actual performance.",,17.0,,0,0
211,1,703,"2010-04-28 23:13:08","My MCE values are not accurate. I'm not predicting from the data, I'm working the competition system. Basically, I know how many entries are used to calculate the public MCE and where they are.<br><br>To get around the selection bias, I plan to determine the correct public MCE and use the extra information to make a more accurate prediction based on the test data. It's a long shot.<br><br>I've known from early on that I won't be winning the competition, but the analysis led to interesting results which should inform future contests.<br><br>For example, if you have 100 yes/no questions, how many guesses are needed to determine the answers with certainty?<br><br>In the worst possible configuration you can get to certainty with about 70 guesses on average (ie - 70% of the number of questions). Most situations are not worst possible, these require <b>fewer</b> guesses.<br><br>The reduction from 100% to 70% was important - with it I can just squeak by determining the entire public MCE and have 6 submissions left over.<br><br>I know all but 9 values of the public MCE, and I'm guaranteed to discover at least one new value with each new submittal.<br><br>An interesting statistic: if you sort the test data on Viral Load and set the first half to ""Responded"" you get a public MCE of 61.0577. If this holds across the entire test set, then I can expect 100% of 30% of the data, and 61.0577% of 70% of the data, for a grand total of 72.6% MCE. Not enough to win :-(<br><br>I <b></b>suspect Alex (""team sayani"") will be the winner.<br>",,18.0,,0,0
212,1,1744,"2010-04-28 23:13:08","This has certainly been an interesting problem.  The sets are clearly not random (as has been pointed out numerous times).  However, even with that knowledge - it has been difficult to turn that to an advantage.  I have broken up the combined data into five different groups and have been trying to come up with separate solutions/theories.  <br><br>Of course - this is made more difficult in that one of the groups appears nowhere in the prediction set - and another is present in the prediction set, but not in the area Rajstennaj mentions that will allow me to get feedback by way of score changes.<br><br>Rt184 is supposed to be very important - and it is in the training set, but there are only 7 variation instances in the entire prediction set in the scoring area.  <br><br>Took me a few times before I figured out why my ""improved"" method was having either no effect - or a negative one.<br><br>I don't think 72.6% is that bad (and you have already mentioned some info that leads me to believe an educated guess would get you closer to ~78%)<br><br>Some of this reminds me of that brain teaser about having 7 coins, a balance, and three weighs to guess which one is fake...",,19.0,,0,0
2,2,606,"2010-04-29 15:48:46","Hi, I'm interested in participating in the contest, however I need to know what tools will actually need to be released upon entry.<br><br>I have access to proprietary software and super computing power, but if our predictions work well we cannot hand over our platform. <br>We can describe it and possibly even release a model that can be simulated by users, but the platform itself is trademarked and licensed.<br><br>Please let me know as I would really like to make a contribution to this project!<br><br>Thanks,<br>-Tanya<br mce_bogus=""1"">",,1.0,,0,0
3,2,478,"2010-04-29 15:48:46","Tanya,<div><br></div><div>Good to hear from you. &nbsp;Since we plan to publish at least the top-entry in a peer-reviewed manuscript we would need a reproducible description of the algorithm. &nbsp;If you can describe it in psuedo-code I'm sure I could implement it in Python for the actual publication. &nbsp;Even if it takes weeks to search the feature-space without your super-computer then that's okay.</div><div><br></div><div>However, if even the algorithm is proprietary and the company isn't willing to allow its publication then I'm sorry but your outta luck.</div><div><br></div><div>-Will</div>",,2.0,,0,0
4,2,368,"2010-04-29 15:48:46","Hi Tanya, <br><br>Kaggle will maintain a rating system. If you win but you're ineligible for prize money, you will still get a strong rating.<br><br>Anthony<br mce_bogus=""1"">",,3.0,,0,0
5,3,634,"2010-04-30 14:32:29","Hi,<br><br>I just read in csv files and I've got a question about the PR and RT sequences. The first one (PR) is 297 characters and the RT is 1476 characters and since I am not from the biomedical field I just wanted to double check.<br><br><br>Regards,<br><br>Alberto<br><br><br>P.S.<br><br>Nice morete seoi nage or perhaps ippon seoi nage (not sure though)<br>",,1.0,,0,0
8,3,478,"2010-04-30 14:32:29","The difference in size between the RT and PR protein segments is correct. &nbsp;These are two different genes in the HIV-1 genome. &nbsp;There are instances in this dataset in which there is no RT or no PR sequence (I've taken out any patients that are missing both). &nbsp;This does not mean that the gene is missing, it means that they didn't measure it. &nbsp;So you'll have to make sure your classifier is robust to missing features.<div><br></div><div>And its an ippon-seonage .. my morote is terrible for both myself and my uke ;)</div>",,2.0,,0,0
41,3,864,"2010-04-30 14:32:29","<meta http-equiv=""content-type"" content=""text/html; charset=utf-8""><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 19px; line-height: 22px; "">I have a question: how do you infer that RT is 1476-chars long? the field length goes from 579 to 1482 in the training data.&nbsp;</span><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 19px; line-height: 22px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 19px; line-height: 22px; "">And what's the reason for having so many incomplete RTs? sequencing errors?</span></div>",,3.0,,0,0
6,4,638,"2010-04-30 15:40:57","It seams that the <b>training</b> dataset contains about 80% of patients not responding to treatment while the <b>test</b> dataset seams to contain around 50% of none responding patients. I hence conclude that the training set is not a uniform sample of the total number of patients. Is this done on purpose ?",,1.0,,0,0
7,4,478,"2010-04-30 15:40:57","The dataset is limited by the total number of HIV-1 samples with sequence and&nbsp;therapeutic&nbsp;information in the public domain. &nbsp;And this dataset has virtually all of them. &nbsp;The entire dataset is skewed towards non-responders since the data is mostly from the 1980's-1990's when most therapies were less effective then they are today.<div><br></div><div>The dataset was intentionally split to ensure that the testing dataset is 50-50 responders vs. non-responders, the training set is everything that remains. &nbsp;This was done to ensure that competitors don't ""artificially"" lower their ""responder"" prediction rate because they know the dataset is biased.</div><div><br></div><div>Some biological datasets are even more biased than this one (protein-protein interaction datasets are highly biased towards ""negative"" data). A common way to ensure that you have a robust classifier is to test against a 50-50 training set in which you have&nbsp;artificially&nbsp;created an even distribution of positive and negative events.</div>",,2.0,,0,0
9,4,673,"2010-04-30 15:40:57","Is the test dataset drawn from the same population as the training dataset? Otherwise the predictions made will not be valid.<br mce_bogus=""1"">",,3.0,,0,0
13,4,650,"2010-04-30 15:40:57","The individuals in the dataset are all humans, are all being recently infected by HIV and they received the same treatment.<br><br>Welcome to the problems faced in science... you can't really know whether the two samples come from the same population, e.g. if the viruses that infected them come all from the same strand, if the indivuals have similar genomes... there are a lot of informations that you won't never know.<br mce_bogus=""1"">",,4.0,,0,0
10,5,673,"2010-05-01 11:00:57","Some of the nucleotide codings - I've spotted an ""N"", a ""Y"" and an ""M"" already - are not from the standard ACGT set.<br><br>What do they represent?<br><br>Also HIV is a retrovirus (hence the Reverse Transcriptase protein). Retroviruses are RNA viruses, which means the are usualyy coded with ACGU not ACGT - the U represents uracil found in RNA instead of thymine:<br>http://en.wikipedia.org/wiki/RNA<br><br><br>",,1.0,,0,0
12,5,650,"2010-05-01 11:00:57","'N' means that a base can be either A,C,G or T, and problems in the sequencing process have not permitted to identify which one is the correct base for sure.<br><br>In the same way, Y means a pyrimidine, which is C or T.<br><br>You can find the correct IUPAC convention here:<br>- http://www.dna.affrc.go.jp/misc/MPsrch/InfoIUPAC.html<br><br mce_bogus=""1"">",,2.0,,0,0
11,6,650,"2010-05-01 11:56:47","my background is from biology and, even if I have been doing bioinformatics for a few years now, I don't have enough knowledge of machine learning to solve this by myself: therefore, if someone is interested in making a two-people team with me, I would be glad to collaborate, provided that you explain the machine learning part to me.<br><br>In any case, since I am more interested in learning than in the prize of the competition, I will put here some ideas for everybody:<br><ul><li>the two sets of sequences represent coding sequences of two proteins; therefore, one thing to do is to translate them and compare the protein sequences. Even if two individuals have different DNA sequences for a gene, they can have the same protein sequences; and since only the protein is exposed to functional constraints, then it will be more interesting to see the differences in the protein sequences.<br></li><li>analyzing k-mers doesn't seem very interesting to me. k-mers are usually used to identify regulatory motifs in DNA, which define when a gene is expressed, how, etc.. However, these signals usually are not inside the coding part of a gene sequence, but rather in the positions before or sorrounding the gene. So, the regulatory factors that you are looking with k-mers could be not included in the sequences given. For a similar reason, the GC content is not so informative.<br></li><li>a possible approach would be to look at which sites are the most variable within the protein sequences.<br></li></ul>",,1.0,,0,0
47,6,882,"2010-05-01 11:56:47","I'm in a similar position.&nbsp; I'm comfortable with the biological concepts but the machine learning is all new to me.<br><br>Judging from your other post it looks like we're both intending to use python as well.&nbsp; It's not exactly the ideal skills match but perhaps there is still some scope for cross-fertilization of ideas.&nbsp; Get in touch if you're interested.&nbsp; Email sent to jonathan @ the domain in my profile should reach me.<br mce_bogus=""1"">",,2.0,,0,0
53,6,703,"2010-05-01 11:56:47","Don't get hung up because you don't know machine learning. Machine learning won't get you anything you don't already know about the problem.<br><br>Machine learning is used to predict patterns in future data given past data, and it's nothing more than an application of concepts you already know.<br><br>To use a machine learning system, you would feed it patients one at a time and have it learn to classify respond and non respond based on prior data. Once trained, you would have it try to classify the test data.<br><br>The algorithm would have poor results at the beginning, and get better with training.<br><br>In this case, we know all the data at the outset, so it makes no sense to train. It will be far more efficient and accurate to just take all the data at once and look for significant features. That's only what a machine learning algorithm would do anyway, but over time and incrementally.<br><br>For example, gradient descent (a machine learning algorithm) is equivalent to linear regression. Since you have all the data, you can get the same output by just calculating the linear regression.<br><br>It will be much more effective to just rummage around in the whole dataset using percentages and statistical inference.<br><br mce_bogus=""1"">",,3.0,,0,0
75,6,808,"2010-05-01 11:56:47","Use a mixture of different models (linear regression, neural networks). Choosing the best model by using the wave criterion. The theoretical grounding of the criterion is based on Bayes Theorem, the methods of cybernetics and synergy. See, article ""Performance criterion of neural networks learning"" published in the ""Optical Memory &amp; Neural Networks"" Vol. 17, number&nbsp; 3. pp. 208-219.&nbsp; DOI:10.3103/S1060992X08030041<br>http://www.springerlink.com/content/t231300275038307/?p=0c94471924774e8894973ad3c0d391a7&amp;pi=0<br mce_bogus=""1"">",,4.0,,0,0
81,6,703,"2010-05-01 11:56:47","I don't have access to SpringerLink.<br><br>Can you post a link to your paper that can be read for free?<br><br>(Alternately - Can someone hook me up to SpringerLink somehow?)<br><br mce_bogus=""1"">",,5.0,,0,0
83,6,808,"2010-05-01 11:56:47","<span id=""result_box"" class=""short_text""><span style="""" title="""">Write to me. </span><span style="""" title="""">I shall send you by e-mail.</span></span>",,6.0,,0,0
14,7,478,"2010-05-02 14:37:35","Now that we have a handful of algorithms that are well above random, does anyone want to post a discussion on their particular algorithm, feature-set, approach, etc.<br><br>I'm hoping to foster an open discussion of the techniques used here. You can post a link to a public repo, a blog post, etc.<br><br>Good Luck,<br>Will&nbsp; <br mce_bogus=""1"">",,1.0,,0,0
15,8,638,"2010-05-04 09:32:54","The goal of the game is to answer the question ""Do patients respond to the treatment ?"". However, I have found almost no information about the aforementioned&nbsp;treatment and drugs the patients were given. Did they all follow the same therapy ? Was the posology strictly equivalent ? What drugs exactly were consumed ?",,1.0,,0,0
16,8,650,"2010-05-04 09:32:54","Look at the description in the Data section:<br><br>""""""These sequences are from patients who had only recently contracted HIV-1
 and had not been treated before""""""<br><br>moreover, I think you can assume that the posology has been respected correctly.<br mce_bogus=""1"">",,2.0,,0,0
17,8,478,"2010-05-04 09:32:54","<blockquote><p>The dataset contains many different therapies.&nbsp; I have
ensured that there is an equal proportion of therapies distributed
between the testing and training datasets.&nbsp; For example, if the
training dataset is 30% AZT users then the testing dataset is also 30%
AZT users.&nbsp; The difficulty with breaking out the dataset by therapeutic
intervention is that the ""cocktail"" of drugs given to each patient is
not universal.&nbsp; There are 13 drugs which are given in combination of
1-3 at a time.&nbsp; If I limited the dataset to the largest dug only (AZT
in this case) then you'd be stuck with a training and testing dataset
of barely 200 patients.</p><p>There have been numerous publications
which can ""to some degree"" predict which therapies will work for which
patients ... based on their viral genome.&nbsp; The more interesting
question is ""Are there markers which indicate good progression
independent of the therapy chosen.&nbsp; I arranged the dataset to
facilitate that question.</p><p>As far as the dosages given to the
patients .. even I don't know that information.&nbsp; I can only assume that
doctors were prescribing the proper dosage.&nbsp; There is an issue with
patient ""compliance"" ... many of the earlier drugs made you pretty sick
(and were very expensive) and so patients would take less then the
recommended dosage so it would last longer.&nbsp; If the study directors
noticed that patients were being non-compliant then they'll often drop
them from the study (since they make the numbers worse), but I have no
data indicating the level of compliance.</p><p>Hope that helps,</p><p>Will</p></blockquote>",,3.0,,0,0
18,9,672,"2010-05-05 04:00:15","Hi there,<br><br>The Fontanelles is a group which does HIV research professionally and so has some specialized information in this area. We're disqualifying our entry, but have put it in just for fun as a target.&nbsp; We may be back if someone beats it.<br mce_bogus=""1"">",,1.0,,0,0
19,9,478,"2010-05-05 04:00:15","Good to hear from you Paul,<br><br>I saw your entry just a little while ago and I noticed that you were listed yourself as a clinical virologist.&nbsp; If you'd like to talk about your methods I'd be glad to hear them.&nbsp; If your interested in writing a guest-post for my blog on this competition feel free to drop me an e-mail.<br><br>Thanks,<br>Will<br mce_bogus=""1"">",,2.0,,0,0
20,10,634,"2010-05-06 13:56:24","Hi guys,<br><br>Apologies in advance if this is a silly question but I feel like a fish out of the water with this PR and RT strings.<br><br><br>I have been spending sometime on the PR and RT sequences and noticed that if I split the sequences in 3-mers I'll get 99 groups for PR (297/3) and 492 for RT (1476/3). My question is whether or not make sense to split the sequences in ternaries. Is there any other alternative, perhaps 2-mers?<br><br>Does it make sense to calculate the odds of&nbsp; responding to the treament for each k-mer or may be re-group them into 2 consecutive k-mers and the calculate the odds?<br><br>Thanks in advance for your help.<br><br>Alberto<br mce_bogus=""1"">",,1.0,,0,0
22,10,703,"2010-05-06 13:56:24","You probably know this, but in case you don't here's some info which might help your analysis.<br><br>The nucleotide sequence is used by the cell to build a protein, and proteins are made up of a string of amino acids.<br><br>The cell structure (ribosome) takes the nucleotide sequence in groups of three letters. Each grouping of three letters indicates which is the next amino acid to attach to the growing chain which makes up the protein. Once the chain is complete, the protein is let go and it folds over and around itself to make a single molecule with a specific shape.<br><br>(I'm glossing over some details. The folding actually happens as the string is being created, and there may be other steps in the process such as chopping off sections after the protein is made.)<br><br>For example, the following nucleotide sequence:<br><br>CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAGGG...<br><br>will be interpreted by the ribosome as this:<br><br>CCT CAA ATC ACT CTT TGG CAA CGA CCC CTC GTC CCA ATA AGG ATA GGG ...<br><br>and the protein generated will be this<br><br>Proline+Glutamine+Isoleucine+Threonine ...<br><br>The lookup tables for these translations can be found in numerous places on the net under the term ""Genetic Code"".<br><br>There are 4 possible nucleotides in 3 positions within such a triplet, giving a total of 64 possible codons. Three of these mean ""Stop"", one of these is conditionally ""Start"", and each of the others indicate one of 20 amino acids.<br><br>This means that there is redundancy in the genetic code: Both TTT and TTC encode Phenylalanine, so two nucleotide sequences which differ in the same position by these codons will generate identical proteins.<br><br>Furthermore, some common-sense logic can be applied to the sequences. If there is a codon with missing information ""TG?"" and it's in the middle, the unknown codon is almost certainly not ""TGA"" because you won't normally have a ""STOP"" codon in the middle of the sequence.<br><br>If you are going to do correlations on the nucleotide sequence as a generic string, you can first translate the sequence into a string of amino acids and work with *that* as a string. This will automatically match the redundancies in the genetic code and result in a string 1/3 as long.<br><br><br>Any biologists who note an error in the previous, please reply with a correction.<br><br mce_bogus=""1"">",,2.0,,0,0
24,10,650,"2010-05-06 13:56:24","Rajstennaj, what you have written is correct.<br><br>I don't believe it is very useful to look at k-mers distribution, it is better to concentrate on variability on certain positions..<br>",,3.0,,0,0
21,11,737,"2010-05-06 21:19:20","Hello,<br><br>We developed string kernels for HIV in 2008, so you might want to check that.<br><br><a mce_href=""http://www.retrovirology.com/content/5/1/110"" href=""http://www.retrovirology.com/content/5/1/110"">http://www.retrovirology.com/content/5/1/110</a><br><br>We are preparing a submission based on that.<br><br>Cheers!<br><br>Mr. Sébastien M. Boisvert, PhD student -- http://boisvert.info/<br><br>edit Thu May&nbsp; 6 18:22:55 EDT 2010: removed irrelevant information.<br mce_bogus=""1"">",,1.0,,0,0
23,12,703,"2010-05-07 20:52:31","<a href=""http://www.OkianWarrior.com/Enjoys/Kaggle/Images/HIV.zip"" mce_href=""http://www.OkianWarrior.com/Enjoys/Kaggle/Images/HIV.zip"">Here</a> is a quickstart package for people to get up and running without a lot of programming.<br><br>It's in perl, you will also need List::Util and Statistics::Basic from CPAN. The data files for this contest are included.<br><br>BasicStats.pl<br><br>This will read in the data and print some basic statistics. You can use this as a framework for your own explorations of the data. The source informs several ways of accessing and manipulating the data.<br><br>TestMethod.pl<br><br>This will randomly select a test set from the training data, then call Train() on the remaining data and Test() on the test data and print out the MCE. Train() and Test() are stubs - rewrite these functions to test your own prediction methods.<br><br>KaggleEntry.pl<br><br>This will read in the test data and the training data, call Train() on the training data, then call Test() on the test data, then generate a .csv file properly formatted for Kaggle submission. Train() and Test() are stubs - rewrite these functions to submit an entry based your own prediction methods.<br><br>There is a more comprehensive README in the package.<br><br>If you find problems, please let me know (via the Kaggle contact link), I will update &amp; repost.<br><br>I expect bugs will be fixed and more functionality will be added over time, updates will be posted here.<br><br>(Please be kind to my server!)<br mce_bogus=""1"">",,1.0,,0,0
31,12,703,"2010-05-07 20:52:31","A new version is available with some enhancements and a minor bug fix, available <a href=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/HIV.zip"" mce_href=""http://www.OkianWarrior.com/Enjoys/Kaggle/Images/HIV.zip"" rel=""nofollow"" rel=""nofollow"" >here</a>.<br><br>A complete description of the changes is included with the package.<br mce_bogus=""1"">

<br>BootstrapMethod.pl<br><br>This will run TestMethod.pl 50 times with different train and test sets, then calculate the mean MCE.<br>Useful if your method has a random component to it. <br><br>
(Per the <a href=""http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29"" mce_href=""http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29"">Wikipedia entry</a> on bootstrapping.)<br><br><br>PlotData.pl<br><br>This will generate several data files from the training data, which can then be displayed using gnuplot.<br>Included are several gnuplot scripts to get you started viewing the data in interesting ways, including this:<br><br><br><img alt=""Viral Load vs. Pct responded"" src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/ViralLoadPct2.gif"" mce_src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/ViralLoadPct2.gif""><br><br><br><br>Enjoy, and let me know if you find problems.<br>",,2.0,,0,0
32,12,728,"2010-05-07 20:52:31","To contribute to The Cause, here are all of the training and test instances translated from DNA to amino acids, and aligned to the same reading frame with the consensus sequences. &nbsp;Also with some basic proteomics data such as molecular weight, pI, and percentage of helix, turn, and sheet segments, derived from ProtParam --&nbsp;<a href=""http://expasy.org/tools/protparam.html"" rel=""nofollow"" >http://expasy.org/tools/protparam.html</a>) &nbsp;You can download the data here:<div><div><br></div><div>http://dl.dropbox.com/u/3966882/hiv/alignments_and_proteomics.csv</div><div><br></div><div>For the non-biologists: a consensus sequence is sort of the ""average"" or ""standard"" sequence within a database of different sequences, and variations from this consensus are called polymorphisms. &nbsp;So in the above file, there are several hyphens within the training/test sequences where the consensus sequence has an amino acid, but the training/test sequence has a deletion there. &nbsp;The consensus sequences for protease and reverse transcriptase are here:</div><div><br></div><div>http://dl.dropbox.com/u/3966882/hiv/consensus.txt</div></div>",,3.0,,0,0
33,12,703,"2010-05-07 20:52:31","Cory's proteomics data is now included in the quickstart package.<br rel=""nofollow"" ><br>(His distinctiveness was added to the collective.)<br><br>A function to read and add the proteomics to the patient data is included, and all sample programs load the new data.<br><br>BasicStats.pl prints simple statistics based on the proteomics - look there to see how to access the new data.<br><br>(But nonetheless it's straightforward.)<br><br>The new version is <a href=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/HIV.zip"" mce_href=""http://www.OkianWarrior.com/Enjoys/Kaggle/Images/HIV.zip"" rel=""nofollow"">here</a>.<br><br>",,4.0,,0,0
48,12,882,"2010-05-07 20:52:31","The site for the quickstart package seems to be down.<br mce_bogus=""1"">",,5.0,,0,0
49,12,836,"2010-05-07 20:52:31","Working&nbsp; for me now <br>",,6.0,,0,0
50,12,703,"2010-05-07 20:52:31","That's my home server - I turn it off at night [EDT] sometimes.<br><br>If it doesn't work, try again 12 hours later.<br><br mce_bogus=""1"">",,7.0,,0,0
51,12,882,"2010-05-07 20:52:31","Working fine for me as well now.<br><br>Thanks.<br mce_bogus=""1"">",,8.0,,0,0
84,12,717,"2010-05-07 20:52:31","Thanks Raj for the quickstart package and Cory for the proteomics data.<br><br>I noticed for Cory's proteomics data two entries are missing just thought I would add them in if<br>they have not been mentioned already.<br><br>Training data Patient id 659 PR sequence<br>CCTCAGATCACTCTTTGGCAACGACCCGTCGTCACAGTAAAGATAGGGGGGCAACTAAAGGAAGCTCTATTAGATACAGGAGCAGATGAYACAGTATTAGAAGACATGAATTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTTGTCAAAGTAAGACAGTATGATCAGGTACCTATAGAATTTTGTGGACGTAAAACTATGGGTACAGTATTAGTAGGACCTACACCTGTCAACGTAATTGGAAGRAATCTGTTGACTCAGATTGGGTGCACTTTAAATTTT<br><br>Translation<br>PQITLWQRPVVTVKIGGQLKEALLDTGADXTVLEDMNLPGRWKPKMIGGIGGFVKVRQYDQVPIEFCGRKTMGTVLVGPTPVNVIGXNLLTQIGCTLNF<br><br>Test data Patient id 674 RT sequence<br>CCCATTAGTCCTATTGAAACTGTRCCAGTAAAATTAAAGCCAGGAATGGATGGCCCAAGAGTTAAACAATGGCCATTGACAGAAGAAAAAATAAAAGCATTAGTAGAAATTTGTACAGAAATGGAAAAGGAAGGAAAAATTTCAAAAATTGGGCCTGAAAATCCATACAATACYCCAGTATTTGCCATAAAGAAAAAGGACAGTTCYANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNTTAGATAAAGACTTCAGAAAGTATRCTGCATTCACCATACCTAGTGTGAACAATGAGACACCAGGGATTAGATATCAGTATAATGTGCTTCCACAGGGATGGAAAGGATCACCAGCAATATTCCAAAGTAGCATGACAAAAATCCTAGAGCCTTTTAGAAAACAAAATCCAGACATAGTTATCTATCAATACATGGATGATTTGTATGTAGGATCTGACTTAGAAATAGGGCAGCATAGAACAAAAATAGAGGAACTGAGAGATCATCTATTGAAGTGGGGACTTTACACACCAGACMAAAAACAYCAGAAAGAACCTCCATTCCTTTGGATGGGTTATGAACTCCATCCTGATAAATGGACAGTACAGCCTATAGTGCTGCCAGAAAAAGACAGCTGGACTGTCAATGACATACAGAAGTTAGTGGGAAAATTGAATTGGGCAAGTCAGATATATCCAGGGATTAAAGTAAGGCAATTATGTAAACTCCTTAGGGGAACCAAAGCACTAACAGAAGTAGTACCATTAACAGAAGAAGCAGAGCTAGAACTGGCAGAAAACAGGGAGATTYTAAAAGAACCAGTACATGGAGTGTATTATGACCCAACAAAAGACTTAATAGCAGAAATACAGAAACAGGGGCTAGGCCAATGGACATATCAAATTTATCAAGAACCATTTAAAAATCTGAAAACAGGAAAGTATGCAARAATGAGGRGTGCCCACACTAATGATGTAAARCAACTAACAGAGGYGGTRCAAAAAATAGCCACAGAAAGCATAGTAACATGGGGAAAGACTCCTAAAYTTAAATTACCCATACAGAAAGAAACATGGGAGGCATGGTGGACAGAGTATTGGCARGCCACCTGGATTCCTGARTGGGAGTTTGTCAATACCCCTCCCTTAGTGAAATTATGGTACCAGTTAGAGAAAGAACCYATAGTAGGAGCAGAAACTTTCTATGTAGATGGGGCAGCTAATAGGGAAACTAAATTAGGAAAAGCAGGATATGTTACTGACAGAGGAAGACAAAAAGTTGTCTCCCTAACGGACACAACAAATCAGAAGACTGAGTTACAAGCAATTAATCTAGCTTTN<br><br>Translation<br>PISPIETXPVKLKPGMDGPRVKQWPLTEEKIKALVEICTEMEKEGKISKIGPENPYNXPVFAIKKKDSXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXLDKDFRKYXAFTIPSVNNETPGIRYQYNVLPQGWKGSPAIFQSSMTKILEPFRKQNPDIVIYQYMDDLYVGSDLEIGQHRTKIEELRDHLLKWGLYTPDXKXQKEPPFLWMGYELHPDKWTVQPIVLPEKDSWTVNDIQKLVGKLNWASQIYPGIKVRQLCKLLRGTKALTEVVPLTEEAELELAENREIXKEPVHGVYYDPTKDLIAEIQKQGLGQWTYQIYQEPFKNLKTGKYAXMRXAHTNDVXQLTEXXQKIATESIVTWGKTPKXKLPIQKETWEAWWTEYWXATWIPXWEFVNTPPLVKLWYQLEKEXIVGAETFYVDGAANRETKLGKAGYVTDRGRQKVVSLTDTTNQKTELQAINLAX<br><br><br>Thanks,<br><br>Jack<br><br mce_bogus=""1"">",,9.0,,0,0
34,18,650,"2010-05-18 16:53:05","There are three sequences that have a stop codon in the middle, so only a portion of them is coding.<br><br>I wonder which is the best way to handle with this. Would you remove the whole sequences from the data? Will you consider the sequence after the stop codon, or not? or maybe it is better to ignore this fact completely?<br><br>I don't know what is the best way to put this information into a machine learning software.<br mce_bogus=""1"">",,1.0,,0,0
35,18,703,"2010-05-18 16:53:05","Some parts of the sequence are highly conserved - they cannot change much (or occasionally, at all) without modifying the function of the protein.<br><br>Look at the same position in all the other sequences. If they all have the same codon, or overwhelmingly most have the same codon, then it's likely that the stop codon is a data error and you can assume it's the overwhelmingly likely case.<br><br>If the codon is in a position which varys widely across all the other samples, then it's in a position which is *not* highly conserved, which means that it's unlikely to matter.<br><br>Also, if you suspect an error in transcription and have more than one possible solution (perhaps all the other sequences have one of two possibilities in that position), you can look at all possible cases of what the codon might be, and then look at the shapes and sizes of the corresponding amino acids.<br><br>For example, you have an error and the possible replacements are Threonine or Tryptophan. If the corresponding codons in the other samples are all Alanine and Serine, then Threonine is the best guess. (Tryptophan is big and bulky, the others are small and similar.)<br><br>Note that in all this, you are finding the <b>most likely</b> answer, not the correct answer.<br><br><br>Any biologists who note an error in the previous, please post a correction.<br><br mce_bogus=""1"">",,2.0,,0,0
36,18,703,"2010-05-18 16:53:05","Also of note, if you read 





Sébastien's paper (from the post about string kernels), they specifically discounted samples that had coding errors.<br><br>From his paper:<br><blockquote><br>Sequences containing #, $ 
or * were eliminated from the dataset. The signification of these 
symbols was reported by Brian Foley of Los Alamos National Laboratory 
(personal communication). The # character indicates that the codon could
 not be translated, either because it had a gap character in it (a 
frame-shifting deletion in the virus RNA), or an ambiguity code (such as
 R for purine). The $ and * symbols represent a stop codon in the RNA 
sequence. TAA, TGA or TAG are stop codons.</blockquote><br><br>http://www.retrovirology.com/content/5/1/110",,3.0,,0,0
37,18,703,"2010-05-18 16:53:05","On the subject of ambiguous data, here are some of my thoughts.<br><br>First of all, the sequences have to be aligned. Once aligned, vast numbers of columns will be highly conserved vertically.<br><br>Given that, we can talk about a particular column within all sequences, with a column being a 3-nucleotide codon. Some sequences are longer/shorter, so some sequences will have blanks at a particular column.<br><br>A codon is a triiplet of (one of four) nucleotides, making a total of 64 possible codons. These code for 20 amino acids, so there is some redundancy. A particular amino acid is likely to be encoded my more than one codon, some of them have 6 codes.<br><br>A first pass might consider duplicate codings as the same. Since they encode the same amino acid, both encodings will generate chemically identical results. One could go through the data and replace all synonyms by some chosen base coding. This will eliminate some of the variation.<br><br>Next, consider a particular column. If the column has the same codon in both data sets, it has no predictive power so it can be eliminated - cut from the sequence. This will shorten the string and make certain computations easier (such as string kernels).<br><br>I've got about 2 more pages of thoughts on the matter. Anyone else want to comment?<br mce_bogus=""1"">",,4.0,,0,0
38,18,650,"2010-05-18 16:53:05","No, I have translated the sequences with the tool transeq from EMBOSS, and I have found some stop codons in the middle of some sequences.<br><br>There is not a specific strand for which all the sequences are completely coding, but with the strand 1 you can translate all the sequences except a few.<br><br>The sequences of the PR protein for individuals 51, 188, 612 and 665 contain a stop codon in the middle, 785. I was asking what is the best approach to handle these cases. Should I consider that the genotype of these sequences is null for all the nucleotides after the stop codon? or should I keep them ignoring the stop?<br><br>@ <a rel=""nofollow"" onfocus=""blur();"" href=""../../../../../Rajstennaj-Barrabas/Profile"" mce_href=""http://kaggle.com/Rajstennaj-Barrabas/Profile"">Rajstennaj Barrabas
						</a>
						: thank you for your feedback: but please, let's try to keep the discussion on the stop codons here, and let's open new discussions for other topics in this forum. <br><br><br><br mce_bogus=""1"">",,5.0,,0,0
39,18,703,"2010-05-18 16:53:05","I'm sorry if my post seemed off-topic. Let me try an example.<br rel=""nofollow"" ><br>
<a href=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/RTrans.html"" mce_href=""http://www.OkianWarrior.com/Enjoys/Kaggle/Images/HIV.zip"" rel=""nofollow"">Here</a> is a list of RTrans sequences for all individuals in the study. Scroll down to patient 408.<br><br>(I've included an excerpt below.)<br><br>The stop codon for patient 408 is at position 12 (shown in red) in the sequence. Looking at that position in all patients, I note that the vast majority of them seem to be AAG. Counting the codons in that position results in the following:<br><pre><br>AAA: 71<br>AAC: 2<br>AAG: 1451<br>AAR: 32<br>AAS: 2<br>ARG: 3<br>MAG: 1<br>TAG: 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;- Stop codon<br>TTA: 1<br>WAG: 1<br>---: 127&nbsp;&nbsp;&nbsp; &lt;- No codon<br></pre><br><br>Most of these are AAG analogues. For example, the ""R"" in AAR above represents A or G, so those 32 entries could reasonably be AAG as well. (And of course, AAA and AAG are synonyms for the same amino acid.)<br><br>My conclusion: given the values in the other samples, and knowing that a stop codon won't appear in the middle, it's reasonable to assume that the stop codon is a data error and that the most likely correct value is AAG.<br><br>The second thing to note is that this end of the sequence is ""ragged"" among all the patients. Many of the sequences begin after this position - they have no codon here.<br><br>This would imply that this position in the sequence is not especially critical to the function of the protein, which gives us circumstantial evidence to believe that changing the TAG to AAG is OK because it won't matter much.<br><br><br>An excerpt from the (very long) HTML file mentioned above:<br><br><br mce_bogus=""1"">

<img alt=""Viral Load vs. Pct responded"" src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/Stop1.gif"" mce_src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/Stop1.gif"">",,6.0,,0,0
40,18,703,"2010-05-18 16:53:05","Here's a question for you.

I found 9 stop codons among 7 patients in the Reverse Transcriptase sequence, but none in the Protease sequence. Your post notes fewer stop codons, in the Protease sequence?<br><br>The IDs seem to match yours - am I using the wrong data? Is RT and PR reversed in the data files?<br> 

<pre><br>--Train Data--<br>51 : RT( 10) = TAA<br>188: RT( 37) = TGA<br>612: RT(203) = TAA<br>665: RT(288) = TAA<br>665: RT(291) = TAA<br>665: RT(294) = TAA<br>785: RT( 27) = TAA<br>--Test Data--<br>408: RT( 12) = TAG<br>437: RT(209) = TAG<br></pre><br mce_bogus=""1"">",,7.0,,0,0
42,18,650,"2010-05-18 16:53:05","Hi,<br>sorry for the delay in answering.<br><br>Don't worry, we have the same results: there are three stop codons in sequence 665, toward the end of the seq; and I didn't calculate anything in the test data.<br>So in total, I have 5 sequences with earlier stop codons in the training data, just like you.<br mce_bogus=""1"">",,8.0,,0,0
44,18,650,"2010-05-18 16:53:05","ok, your idea to threat those cases as sequencing errors is nice, but at least in 665 they should be not errors, since there are three stop codons in a close position. Let me think about it..<br mce_bogus=""1"">",,9.0,,0,0
52,18,703,"2010-05-18 16:53:05","For the case of patient 665, note that the length of the RT sequence is not a strict multiple of three. Examining the alignment of the tail end of the sequence against the other sequences, I note that it would line up very well and eliminate the stop codons if an extra nucleotide is inserted.<br><br>Comparing against other sequences, I edited my input data as follows:<br><br><pre>AAAGTAAAGSATTATGTAAACTCRTTAGGGGAACCAAAGCACTAACAGAAGTAATACCATTAACA"",5,78<br>AAAATAAGGCAATTATGTAAACTCCTTAGGGGAGCCAAAGCATTAACAGAAGTAATACAGTTAACGAAAGAAGCAGAG"",3.3,447<br><br><br>AAAGTAAAGSA<font color=""red"">A</font>TTATGTAAACTCRTTAGGGGAACCAAAGCACTAACAGAAGTAATACCATTAACA"",5,78<br>AAAATAAGGCAATTATGTAAACTCCTTAGGGGAGCCAAAGCATTAACAGAAGTAATACAGTTAACGAAAGAAGCAGAG"",3.3,447<br><br></pre>
<br><br>That's just my take. Also, this is at the ragged end section which is not strongly conserved, so chances are good that any changes I make are not important.<br><br>What are your thoughts on this?<br>",,10.0,,0,0
54,18,650,"2010-05-18 16:53:05","The problem is that a deletion of 1 base is also a possibility in nature, and given the fact that we are talking about HIV, it won't be so strange.<br><br>The description of the data in this competition doesn't say anything about the quality of the sequences, and I am not sure whether can argue that there are errors in there. I thought we could assume that the sequences are right, especially given the fact that this is not a real-data problem. From another point of view, the only thing we know is that HIV is highly variable and accumulates a lot of mutations, and for the case of 665 the deletion is toward the end of the sequence and likely to not have consequences on the protein structure.<br mce_bogus=""1"">",,11.0,,0,0
58,18,478,"2010-05-18 16:53:05","I just wanted to chime in here about the ""stop codon"" issue and the discussion of sequencing errors.<div rel=""nofollow"" ><br></div><div>These are sequences from real patients (not simulated sequences).&nbsp;There is an issue with possible sequencing errors but this is an unlikely explanation for finding these stop-codons. &nbsp;Sequencing errors usually result in&nbsp;ambiguous&nbsp;characters like 'N', 'Y', etc.</div><div><br></div><div>In my research I tend to artificially remove these sequences since they are difficult to interpret. &nbsp;Its unclear whether this is actually a mis-sense mutation, a sequencing error, or a poor sampling of the ""<a href=""http://gateway.nlm.nih.gov/MeetingAbstracts/ma?f=102178611.html"" mce_href=""http://gateway.nlm.nih.gov/MeetingAbstracts/ma?f=102178611.html"">quasispecies</a>"". &nbsp;I included them in the dataset in-case anyone had a brilliant idea on how to deal with them.</div><div><br></div><div>Hope that helps,</div><div>Will Dampier</div>",,12.0,,0,0
80,18,924,"2010-05-18 16:53:05","Is the correct interpretation of HIV variability and the quasispecies issue that:<br><br>With respect to the ""validity"" of individual nucleotide/codons - since the HIV virus generates many variants in a single infected patient in the 
course of one day, it would be correct to view the single sequence associated with each patient as a sample from an ever-changing population of virus within that patient.<br><br>If that is true, these independent variable data could be viewed as a sample of the patient's viral population, or as possessing error-in-variables. As such, I'm tempted to try not to discard the entire data point. Rather, it would seem to me that a predictive model that recognizes the viral plasticity would be preferred.<br><br>On a side note, Will's quasispecies reference states ""that the molecular biologist will be able to provide a molecular 
description of HIV induced disease seems remote"" - has that view changed over the intervening 20 years?<br><br>Thanks in advance for educating a non-biologist<br>",,13.0,,0,0
82,18,703,"2010-05-18 16:53:05","In the old days, humans did sequencing by looking at (images of) spots on gels. My guess is that the contest data could be of the older sort and prone to human data entry errors.<br><br>(There's only 12 of these in the entire dataset, and all of them are in sequence areas which are unlikely to matter.)<br><br>As far as ambiguous codons go, my take is that since the genome is so plastic, any sample will necessarily contain multiple genotypes. In that model, an ambiguous codon might represent <b>both</b> species at once.<br><br>For example:<br><br>ARA &lt;- AAA Lysine AGA Arginine<br><br>Both genotypes in the original sample in roughly equal numbers would cause this type of ambiguity. A good correlation method should give weight to both possibilities.<br><br>Any Biologists want to confirm or deny?<br><br mce_bogus=""1"">",,14.0,,0,0
43,19,650,"2010-05-24 09:23:11","I prefer to program in python if I can. Can you recommend me any good library for machine learning in python?<br><br>I have found <a mce_href=""http://pypi.python.org/pypi/pcSVM/pre%201.0"" href=""http://pypi.python.org/pypi/pcSVM/pre%201.0"">pcSVM</a>, but it seems to be not online anymore. So, I don't know libraries to play with support vector machines in python.<br><br>For Neural network there is <img src=""http://pypi.python.org/pypi/ffnet/0.6.2"" mce_src=""http://pypi.python.org/pypi/ffnet/0.6.2""> which looks good, but I haven't tried it yet.<br mce_bogus=""1"">",,1.0,,0,0
45,19,650,"2010-05-24 09:23:11","<a mce_href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" href=""http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/"">LIBSVM</a>, with interfaces to Python and other languages.<br mce_bogus=""1"">",,2.0,,0,0
46,19,882,"2010-05-24 09:23:11","I also prefer using python.&nbsp; I haven't used any of the following (yet) but might give you some additional options to look into.<br><br><a mce_href=""http://pybrain.org/"" href=""http://pybrain.org/"">http://pybrain.org/</a><br><a mce_href=""https://mlpy.fbk.eu/"" href=""https://mlpy.fbk.eu/"">https://mlpy.fbk.eu/</a><br><a mce_href=""http://pyml.sourceforge.net/"" href=""http://pyml.sourceforge.net/"">http://pyml.sourceforge.net/</a><br><br>You're probably already aware of it but there is also <a mce_href=""http://biopython.org/wiki/Main_Page"" href=""http://biopython.org/wiki/Main_Page"">Biopython</a> which while not a machine learning library should be useful.<br mce_bogus=""1"">",,3.0,,0,0
55,20,650,"2010-05-24 15:19:35","This post is not related to the HIV Progression contest, but it is to send feedback about the Kaggle website.<br><br>First of all, you should really use a better code for the forum... it is very uncomfortable to write here, and there are a lot of templates out there that work better.<br><br>The second point is a more general complaint about the fact that having a prize for solving the competition reduces a lot the opportunity to collaborate to solve the problem together with other people. For example, I have some good ideas on which informations I could use to write a nice machine-learning method to make the prediction... but I am restrained from exaplaining them here because I won't obtain any credit from it :-(<br><br>You should think of a way to reward the people most active in the forum, or in any case you have to reward those that collaborate the most and are more open to the dialogue.<br mce_bogus=""1"">",,1.0,,0,0
57,20,882,"2010-05-24 15:19:35","You might want to send this to the developers directly using the 'contact us' at the bottom right of the screen as well.&nbsp; I contacted them a couple of hours ago about a small bug and got a reply back saying it had been fixed minutes later.<br><br>I doubt they can fix all bugs as quick and feature requests will also take longer but they're definitely responsive to feedback.<br mce_bogus=""1"">",,2.0,,0,0
59,20,703,"2010-05-24 15:19:35","I'll second that.<br><br>Anthony, the Kaggle person who deals with site feedback, is very accessible and open to suggestions.<br><br>And he doesn't get angry or put out even if your complaints are snarky. :-)<br><br>(I know this because I've sent 20 E-mails over the last month suggesting improvements and pointing out things.)<br><br>You can contact him from the ""ask us directly"" link under the help page.<br><br mce_bogus=""1"">",,3.0,,0,0
63,20,650,"2010-05-24 15:19:35","Thank you for answering me. <br><br>I will send an email to them when I 
will have time, but I also like when feedback is visible to everyone.<br><br>Do
 you agree with the fact that it may be not convenient to someone to 
collaborate in the forum? That collaboration should be encouraged more?",,4.0,,0,0
67,20,478,"2010-05-24 15:19:35","<span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">Giovanni,</span><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">I've been trying to stimulate discussion about techniques as much as possible however I seem to be shouting into the dark ... as you can see from the empty forum threads on ""Technique discussion"". I was envisioning that people would have public repos that others commented on, modified, etc. but alas. &nbsp;Apart from a mention of ""String Kernels"" which have yet to make an appearance on the leaderboard ;) and a quickstart package made by&nbsp;Rajstennaj there hasn't been much discussion.</span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">It seems people are willing to discuss questions about the data, since that's helpful to everyone, but exact implementations are lacking.</span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">Maybe this post will encourage some people :)</span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">Will</span></div>",,5.0,,0,0
68,20,368,"2010-05-24 15:19:35","<span class=""Apple-style-span"" style=""font-family: Helvetica,Arial,Geneva; font-size: 13px; line-height: 15px;"" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">Giovanni,<br><br>Thanks for your feedback. Using the forum to give feedback is a good idea. It allows others to see and comment on suggestions. We might set up a proper feedback forum, but for the moment this topic will have to suffice. <br><br>I also agree that the forum is a bit clunky. However, we have a large list of feature requests and only limited resources for the moment - it might take us some time to address this. Apologies. <br><br>I don't think the prize money in this competition is that relevant (the prize is relatively small). Correct me if I'm wrong but I think contestants are driven by </span><span class=""Apple-style-span"" style=""font-family: Helvetica,Arial,Geneva; font-size: 13px; line-height: 15px;"" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">intrinsic</span><span class=""Apple-style-span"" style=""font-family: Helvetica,Arial,Geneva; font-size: 13px; line-height: 15px;"" mce_style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""> factors.<br><br>A ""karma"" system that rewards forum posts is a good idea. Again, apologies for any delay in implementing this, there are lots of features on our ""to do"" list.<br><br>Anthony<br></span>",,6.0,,0,0
69,20,703,"2010-05-24 15:19:35","Any public collaboration would reduce a team's chance of winning the contest.<br><br>Presumably, a solution requires discovering a set of features with predictive value. Also presumably, these are hard to find, so it's likely that any one team will only find a subset of all predictive features.<br><br>A team will get no benefit from making a feature publicly known, and doing so risks making another team's score better (if the other team was unaware of the feature).<br><br>This is a game theoretic result. The Nash equilibrium is for no team to make features publicly known.<br><br>On the other hand, there is some incentive for teams to collaborate privately. Two teams which are #2 and #3 on the leaderboard could connect in private and agree to share their findings. If they agree to split the prize, then they increase their chances of getting 50% reward, which is better than their individual 0% chance of getting the entire reward.<br><br>(This will be true for any set of teams which do not include first place.)<br><br>Collaboration itself takes time and effort, and it's unclear to me whether $250 is worth the trouble. Most people will probably just lose interest rather than make a concert effort to win.<br><br>If you want people to collaborate, then you should set up the system goals to encourage it. Perhaps a prize for most prolific or best collaboration effort or something.<br><br>Note that there is an incentive for the winning team to tell you all the features they discovered, but no incentive for 2nd or 3rd place or any of the others. If your goal is to discover new features for science, the contest setup is not optimal.<br><br>==========================================================================<br><br>That being said, the flip side is to consider the goals from the point of view of the entrants.<br><br>I imagine that most people have entered the contest with the single goal of winning. There's nothing wrong with this, but note that with 28 entrants on the leaderboard (currently), there is a strong likelihood that any individual team will not win.<br><br>Many of these teams haven't made an entry in the last week, some only made one entry.<br><br>If the only goal is to win the contest, most teams will quickly come to the conclusion that they won't be the winner, or that the payoff is not worth the effort, and such like. I expect many teams will eventually drop out.<br><br>On the other hand, if you have goals which can be met by *entering* the contest, if your goals can be met in the process and not in the destination, then you will most likely see through to the end.<br><br>I'm in the latter category. I had a number of goals which could be met by just entering the contest, plus one goal to win.<br><br>(Why I'm outspoken in the forum.)<br><br>",,7.0,,0,0
72,20,953,"2010-05-24 15:19:35","Guys,&nbsp;<div><br></div><div>While I am a newbie on the site; it feels like the site is extremely slow. Not sure what kind of servers/network you are on, but you should definitely look at improving the response times.&nbsp;</div><div><br></div><div><br></div>",,8.0,,0,0
73,20,368,"2010-05-24 15:19:35","Manish, thanks for the feedback. <br><br>The site is hosted on an Amazon EC2 server on the east coast of America.It's a fast server but the site has been more popular than we expected.<br><br>We're currently working on speeding up the site by reducing the number database queries. We may have to implement auto scaling if the site keeps growing so rapidly. <br><br>Anthony<br><br mce_bogus=""1"">",,9.0,,0,0
74,20,368,"2010-05-24 15:19:35","Just made a change which should speed things up. Let me know if it has made a difference for you.<br mce_bogus=""1"">",,10.0,,0,0
77,20,882,"2010-05-24 15:19:35","&gt; The site is hosted on an Amazon EC2 server on the east coast of 
America.It's a fast server but the site has been more popular than we 
expected.<br><br>As problems go that's a good one to have.&nbsp; Congratulations.<br><br>&gt; We're currently working on speeding up the site by reducing the number 
database queries. We may have to implement auto scaling if the site 
keeps growing so rapidly.<br><br>Most of the pages on the site are fairly static so caching those database queries should make a massive difference.&nbsp; The forum is the most dynamic location and even there you're getting 10 to 100 reads for every write.<br><br>Would it not be quite difficult to dynamically scale the database?&nbsp; You would need to start the new database, copy across the complete database from the master to the slave and then re-route the database queries.&nbsp; Given the fickle nature of visitors from social media sites (where I guess most of your spikes in traffic originate from) auto-scaling could be useful for the apache servers once as many of the database reads as possible are cached.<br><br>Speaking of apache, a common approach to squeezing a bit more performance out of a web server is to stick nginx in front of apache to serve the static content and act as a reverse proxy.&nbsp; Have you considered this?&nbsp; You could also try serving your static resources with s3 or cloudfront.&nbsp; The bandwidth charges appear to be the same as ec2 (although the asian cloudfront edge locations are more expensive) and it would relieve the pressure on your servers.&nbsp; Particularly with social media spikes when your visitors will have unprimed caches.<br>",,11.0,,0,0
78,20,368,"2010-05-24 15:19:35","<div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">Jonathan, thanks for your feedback (x2). We're currently working on caching database queries. There are a lot of good suggestions here that we'll try before autoscaling.&nbsp;</span></div>",,12.0,,0,0
79,20,882,"2010-05-24 15:19:35","No worries.&nbsp; Semi-intelligent sounding suggestions are easy.&nbsp; It's actually implementing them which is the hard bit.&nbsp; Good luck!<br mce_bogus=""1"">",,13.0,,0,0
56,21,650,"2010-05-24 15:21:18","ok, I am a good person, so I am going to post this here... hoping that somebody will respond with a similar level of feedback and maybe collaborate with me to solve this competition. <br><br>A nice hint to help solving the competition is this table/database:<br>- http://hivdb.stanford.edu/cgi-bin/PositionPhenoSummary.cgi<br><br>It shows the list of all the positions that are known to be associated with resistance to an HIV treatment, one of AZT, D4T, TDF, ABC, DDI, DDC, 3TC. You see that not all the positions in the sequences are equally important, and it is not always true that the positions that vary the most are more correlated with resistance. It is probable that these positions correspond to key aminoacids in the sequence, that have a key structural role or participate to the catalytic site of the protein.<br><br>My original approach was to use this table to write a machine-learning based software using these inputs, since using all the positions in the sequences would be too cpu-consuming.<br><br>As I was saying in a previous post, I am not interested in winning the prize of this competition, but I would like to learn from people expert in machine-learning methods... I think I could find other applications for these methods to other biological problems, if I learn how to use them properly. So please, don't be shy with the feedback now :-)<br mce_bogus=""1"">",,1.0,,0,0
60,21,703,"2010-05-24 15:21:18","This is an excellent resource, thank you.<br><br>I am wondering, though, is this within the scope of the competition? Are we allowed to use it?<br><br>To win the competition, a prediction method would have to justify the weight given to each component of the decision process. Are we allowed to say ""this piece is weighted highly because of that external data""?<br><br>Cory's proteomics data appears to be information calculated from the given sequences - molecular weight, for instance. That's probably OK for the contest.<br><br>Fontanelles disqualified themselves by having specialized information.<br>
<br>
<br>Will - could you post a reply clarifying the issue?<br><br mce_bogus=""1"">",,2.0,,0,0
61,21,478,"2010-05-24 15:21:18","I'm perfectly happy with using outside information like known HIV-1 resistance mutations, functional annotations or anything else you can think of.<br>",,3.0,,0,0
62,21,703,"2010-05-24 15:21:18","I am dismayed by Will's response.<br rel=""nofollow"" ><br>This is no longer ""no knowledge of biology is necessary to succeed in 
this competition"", the results will be dominated by companies and 
experts which have gleaned information from patients outside of the 
dataset.<br><br>For example, in the database cited:<br><br>63568 RT Sequences, 63842 Protease Sequences<br><br>This database has many patients outside of the contest dataset, and experts have been poring over it making their conclusions publicly available.<br><br>(As for example <a href=""http://wanglab.ucsd.edu/html/publications/HIV_drug_resistance_PNAS.pdf"" mce_href=""http://wanglab.ucsd.edu/html/publications/HIV_drug_resistance_PNAS.pdf"">here</a>.)<br><br>I'd like to make my own conclusions from the data. That's what the contest is about.<br><br>This seems at odds with the statement of the contest: ""This contest requires competitors to predict the likelihood that an HIV 
patient's infection will become less severe, given a small dataset and 
limited clinical information.""<br><br>",,4.0,,0,0
64,21,650,"2010-05-24 15:21:18","Hi Rajstennaj, I understand you but consider that this is the common problem faced by bioinformaticians every day. To do bioinformatics, you have to know both biology and computer science, otherwise it is very difficult to obtain useful results. This is the reason why I came here in this forum to look for help: a good scientist knows that big problems can not be solved by a single mind, you have to interact with people with different skills if you want to obtain real results.<br><br>You can also approach this competition without knowing anything of biology. I think that it will be very interesting to see if programs written without an a priori knowledge of the problem will perform better than those that make use of these informations. The informations stored in that database are derived from observations made with respect of certain HIV therapies, and it is not certain that they will be applicable to the therapy studied in this competition.<br><br><br>",,5.0,,0,0
65,21,478,"2010-05-24 15:21:18","<span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">Rajstennaj,</span><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">I don't see the distinction between knowing the translation matrix of nucleotides to amino-acids and finding a database which implies that specific regions are more important than others. &nbsp;The Stanford database (and their automated annotation webtool) referenced above is in the top google results when you search for ""HIV Therapy prediction techniques"". &nbsp;I imagined people would stumble upon this website (or any other that could be found from a simple google-search) and use the results as just another featureset in the prediction methods.</span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; ""><br></span></div><div><span class=""Apple-style-span"" style=""font-family: Helvetica, Arial, Geneva; font-size: 13px; line-height: 15px; "">I would be surprised by any machine-learning researcher who didn't do even a general survey of current techniques, available datasets, data transformation and normalization methods that apply to the field.</span></div><div><font class=""Apple-style-span"" face=""Helvetica, Arial, Geneva"" size=""3""><span class=""Apple-style-span"" style=""font-size: 13px; line-height: 15px;""><br></span></font></div><div><font class=""Apple-style-span"" face=""Helvetica, Arial, Geneva"" size=""3""><span class=""Apple-style-span"" style=""font-size: 13px; line-height: 15px;"">The other reason I'm not worried about the knowledge of mutation regions is that the techniques that solely use this data barely reach 65% accuracy on this dataset. By reducing the information in the sequence to a vector of ~20 binary calls (most of which have negligible correlation with the response variable) you will ultimately have difficulty fitting a model ... trust me I've tried.</span></font></div>",,6.0,,0,0
66,21,650,"2010-05-24 15:21:18","Anyway, I take the opportunity to say that the link given to the lanl.gov database in the Background section of this competition is wrong. The right link should be http://www.hiv.lanl.gov/content/sequence/HIV/mainpage.html , and once you are on that site, be sure to look at the Sequence compendium http://www.hiv.lanl.gov/content/sequence/HIV/COMPENDIUM/compendium.html .<br><br><br>I have already contacted the author of the lanl.gov database and they told me that it is not longer maintained. It is better to use the Stanford's one: <br>- http://hivdb.stanford.edu/<br><br>I will tell the maintainers of the lanl.gov database about this website, let's see if they will come in this forum.<br mce_bogus=""1"">",,7.0,,0,0
86,21,673,"2010-05-24 15:21:18","<span class=""Apple-style-span"" style=""font-family: Helvetica,Arial,Geneva; font-size: 13px; line-height: 15px;"">Hi Rajstennaj,<br>The most important reason that detailed knowledge of drug resistant mutations will not be much help is that we are not given the treatment the patients received, nor even told if they received treatment.<br>Regards,<br>Bruce<br></span>",,8.0,,0,0
70,22,926,"2010-05-26 08:11:51","How do you explain the correlation between resp and patient id?<br mce_bogus=""1"">",,1.0,,0,0
71,22,650,"2010-05-26 08:11:51","Just random, there are no biological consequences from that correlation.<br mce_bogus=""1"">",,2.0,,0,0
85,23,882,"2010-05-29 20:58:32","Inspired by the work of <a mce_href=""http://kaggle.com/view-postlist/forum-1-hiv-progression/topic-12-quickstart-package/task_id-2435"" href=""http://kaggle.com/view-postlist/forum-1-hiv-progression/topic-12-quickstart-package/task_id-2435"">Rajstennaj 
Barrabasin developing a perl quickstart package</a> I have just finished putting together a python quickstart package.<br><br>Details are available on <a mce_href=""http://jonathanstreet.com/blog/kaggle-predict-hiv-progression"" href=""http://jonathanstreet.com/blog/kaggle-predict-hiv-progression"">the blog post I've just put up</a>.&nbsp; If you have any questions or find any bugs post them here or on the blog and I'll try to answer / fix them.<br><br>I hope it is useful to some of you.<br>",,1.0,,0,0
87,24,978,"2010-05-31 13:30:13","Hi all,<br><br>I'm new to this contest, and am participating not as a serious competitor but just to get familiar with genetic analysis and machine learning. I've been playing around with the dataset, and I see viral load at t0 is highly correlated with survival chances. I've implemented this in my entry (and nothing else), but I still don't get above guessing. Did I do it wrong or is something else going wrong?<br><br>Thanks,<br><br>Coffin<br mce_bogus=""1"">",,1.0,,0,0
88,24,728,"2010-05-31 13:30:13","I haven't done that exact comparison, but you _should_ be getting above random. &nbsp;The test set is split very close to 50/50 between responders and non-responders, so the first place I'd start is to make sure that your submissions (both the random and the VL) also use a 1:1 ratio of responders to non-responders, just to make sure you're comparing apples to apples.",,2.0,,0,0
94,24,703,"2010-05-31 13:30:13","Sort the test data by viral load, then set the 1st half to respond and the 2nd half to non-respond.<br><br>That will get an MCE of 61.0577.<br><br>You might be interested in the graph (reproduced below) posted as part of the quickstart package. This was calculated from the training data, which has 20% responded versus 80% non-responded. Note that the test data is 50/50.<br><br>&nbsp;<img alt=""Viral Load vs. Pct responded"" src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/ViralLoadPct2.gif"" mce_src=""http://www.okianwarrior.com/Enjoys/Kaggle/Images/ViralLoadPct2.gif"">",,3.0,,0,0
